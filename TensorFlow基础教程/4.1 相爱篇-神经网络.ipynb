{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enormous-hundred",
   "metadata": {},
   "source": [
    "# 📚 4.1相爱篇-神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-algeria",
   "metadata": {},
   "source": [
    "         ✅歌很好听，风很舒服，星星也很美，但更多的开心是来自你\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-textbook",
   "metadata": {},
   "source": [
    "# 一、本节目标\n",
    "        本节将详述概述感知机，全连接层，神经网络，在tensorflow2的环境中，用简单的神经网络实现MNIST手写数据识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-variance",
   "metadata": {},
   "source": [
    "# 二、神经网络\n",
    "      在深度学习领域，从人工神经网络到卷积神经网络(CNN)再到深度网络(DNN)，出现的这些视图模拟 人类的大脑对图像的 认知 能力。\n",
    "     首我们先了解了解神经网络的基本组成单元--感知机。如下图:可以看到感知机是全连接层的一部分。感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数(Activation function)后得到活性值(Activation):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "further-logan",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG947.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-editing",
   "metadata": {},
   "source": [
    "    𝑎 = 𝜎(𝑧) = 𝜎(𝒘𝑇𝒙 + 𝑏)，接下来举一下例子:\n",
    "我们令w=（w1,w2),x=(x1,x2),    z=(w1,w2)T*(x1,x2)   ，再带入激活函数，进行计算。\n",
    "\t但是神经网络区别于一般的机器学习算法，神经网络由大量的神经元构成并且添加了非线性的激活函数，可以用于对任意函数的进行估计和近似的数学模型，而任意函数所需要的学习参数，无需人为的干扰就可以自动实现。这里从隐藏层开始，输出层结束 ，来计算神经网络的层数，下面以三层神经网络为例\n",
    "     三层的神经网络整体框架:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-compromise",
   "metadata": {},
   "source": [
    "简单张量的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "convertible-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建W,b张量\n",
    "import tensorflow as tf\n",
    "x=tf.constant([[1,1]],dtype=tf.float32)  \n",
    "w1 = tf.Variable(tf.random.truncated_normal([2, 1], stddev=0.1))\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "unnecessary-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acute-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = tf.matmul(x,w1) + b1\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "supreme-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y = tf.nn.relu(Y) # 激活函数\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-software",
   "metadata": {},
   "source": [
    "# 层的实现\n",
    "    TensorFlow 中有更加高层、使用更方便的层实现方式:layers.Dense(units, activation)， 只需要指定输出节点数 Units 和激活函数类型即可。输入节点数将根据第一次运算时的输入 shape 确定，同时根据输入、输出节点数自动创建并初始化权值矩阵 W 和偏置向量 b。其中 activation 参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "separated-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant([[1,1]],dtype=tf.float32)\n",
    "from tensorflow.keras import layers # 导入层模块\n",
    "fc = layers.Dense(1, activation=tf.nn.relu)\n",
    "Y = fc(x)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "mediterranean-grammar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       "array([[ 0.09156251],\n",
       "       [-0.26234794]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "alternative-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cardiovascular-theorem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[ 0.09156251],\n",
       "        [-0.26234794]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.trainable_variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indonesian-reducing",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG948.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-simple",
   "metadata": {},
   "source": [
    "    任务描述: 用神经网络完成分类任务，假设训练集以四个数据点为例，(1,1)(-1,1)(-1,-1)(1,-1)，对应的标签为1，2，3，4个象限，寻找这四个点的特征，现在有一个新的坐标点(2,2),预测属于第几个象限。\n",
    "    预测结果: 坐标(2,2）预测输出1象限，跟真实标签1做比较看是否 预测正确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-digit",
   "metadata": {},
   "source": [
    "## 2.1 输入层\n",
    "     这里面有四个样本，分别是X=(1,1)(-1,1)(-1,-1)(1,-1)，每一个样本有两个属性，首先将一个样本放入三层神经网络的输入口。这里(x1,y1)是一个样本，与MNIST图像作比较,相当于一张32*32的图像，x1表示一个样本的第一特征值，与MNIST图像作比较,相当于一张二维的32*32的图像拉成784一维向量的第一个像素值，y1表示一个样本的第二特征值，与MNIST图像作比较,相当于一张二维的32*32的图像拉成784一维向量的第二个像素值，这里面一共有784个特征的像素值。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aggressive-baghdad",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG949.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-regression",
   "metadata": {},
   "source": [
    "张量的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "excess-court",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建W,b张量\n",
    "import tensorflow as tf\n",
    "x=tf.constant([[1,2]],dtype=tf.float32) \n",
    "\n",
    "#x=tf.constant([[1,1]])  \n",
    "w1 = tf.Variable(tf.random.truncated_normal([2, 6], stddev=0.1))\n",
    "w1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "middle-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 2.]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "attended-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = tf.Variable(tf.zeros([6]))\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-basement",
   "metadata": {},
   "source": [
    "## 2.2 从输入层到隐藏层\n",
    "    连接输入层和隐藏层的W1和b1。经过矩阵计算H1=X*W1+b1,输入层1行两列*权重两行六列，这样第一层神经网络输出为(1*6)的矩阵。将得到的每一个数值送入到激活函数,这个激活函数主要是加入非线性，增强拟合程度。输出的数值送入到下一层的隐藏层中。常使用的激活函数以下几种:\n",
    "    • 饱和激活函数：sigmoid、tanh\n",
    "    • 非饱和激活函数: ReLU、Leaky Relu、ELU【指数线性单元】、PReLU【参数化的ReLU 】、RReLU【随机ReLU】\n",
    "     具体激活函数可去相恋篇-keras激活函数，详细了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "technological-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = tf.matmul(x,w1) + b1\n",
    "h1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aware-thickness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.        , 0.21352655, 0.        ,\n",
       "        0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = tf.nn.relu(h1)\n",
    "h1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-gardening",
   "metadata": {},
   "source": [
    "层的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "desperate-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant([[1,1]],dtype=tf.float32)\n",
    "from tensorflow.keras import layers # 导入层模块\n",
    "fc1 = layers.Dense(6, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-throat",
   "metadata": {},
   "source": [
    "## 2.3 从隐藏层到隐藏层\n",
    "    和2.2 运算一样，经过矩阵计算H2=H1*W2+b2,隐藏层1行六列*权重六行五列，这样第二层神经网络的隐藏层为(1*6)的矩阵。将得到的每一个数值送入到激活函数中进行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-luther",
   "metadata": {},
   "source": [
    "层的实现方式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "global-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = layers.Dense(5, activation=tf.nn.relu) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-wholesale",
   "metadata": {},
   "source": [
    "## 2.4  从隐藏层到输出层\n",
    "根据激活函数输出的数值，可以得到不同的数值，但是由于是想输出概率，找到概率最大的所对应的类别。所以就使用了Softmax函数，用于输出类别的概率大小。这样就可以预测出这个样本所属于哪个分类，但是怎么衡量预测的好坏，也就是预测输出与真实的标签之间的差别大小，一个好的网络都是靠loss函数来平衡预测的真实度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-canadian",
   "metadata": {},
   "source": [
    "层的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "controversial-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc3 = layers.Dense(4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "committed-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以直接这样写\n",
    "h1 = fc1(x)\n",
    "h2 = fc2(h1)\n",
    "Y = fc3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "advised-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#也可以使用Sequential容器封装为一个网络类\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Dense(6, activation=tf.nn.relu),\n",
    "    layers.Dense(5, activation=tf.nn.relu),\n",
    "    layers.Dense(4, activation=None),\n",
    "])\n",
    "Y=model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-thompson",
   "metadata": {},
   "source": [
    "## 2.5 反向传播\n",
    "以上是正向的传播过程，接下来，就是不断迭代更新网络中所遇到的参数，根据目标函数,优化目标求导，进行链式求导，从而更新W和b中网络参数的值，直到损失越来越小，得到我们满意的值为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-freeze",
   "metadata": {},
   "source": [
    "### 优化目标 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-maryland",
   "metadata": {},
   "source": [
    "   前向传播的最后一步是完成误差的计算，一般 L=g(F(x),y),L是误差计算，也就是损失函数，F(x)里面有很多的参数，需要神经网络进行计算，通过损失不断的优化，我们可以根据损失函数，也就是优化目标，我们的目标是使得输出与预测的值之间产生的误差足够少。一般采用误差反向传播算法求解网络参数𝜃的梯度信息，利用梯度下降法迭代更新：利用误差反向传播算法进行反向计算的过程也叫反向传播(Backward propagation)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-stevens",
   "metadata": {},
   "source": [
    "# 三 、实战\n",
    "  借助MNIST来对神经网络进一步知识巩固"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-direction",
   "metadata": {},
   "source": [
    "MNIST网络架构模型："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "composed-crystal",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG950.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-sigma",
   "metadata": {},
   "source": [
    "## 3.1 数据集介绍\n",
    "       在TensorFlow 2中，已经接纳Keras作为推荐的构建模型的方式，它简单、快捷的同时又具有相当的灵活扩展性，目前通过tf.keras提供内置的全面支持。\n",
    "      在tf.keras 中有两个重要的概念： 模型（Model） 和 层（Layer） 。Layer就是我们常说的深度网络中的层，它封装了具体的计算流程和对应的变量，例如常见的全连接层Dense、卷积层CNN、池化层Pooling等。Model则是一系列层的连接和组织，作为一个整体完成对深度网络整体的描述，是数据通过输入经过各种层和计算得到输出的整个逻辑。对于一个训练好的模型，我们可以直接通过y_pred = model(X) 完成对X样本的深度网络预测。\n",
    "     在tf.keras.applications这个包下，内置了众多现有的图像相关的网络结构以及预训练权重，可以快速的实现图像相关模型的搭建。在tf.keras.layers这个包下内置了深度学习中常用的预定义层，同时也可以通过继承tf.keras.Layer或者Lambda表达式实现我们自定义的层。\n",
    "下面主要以MNIST数据集作为介绍。\n",
    "      MNIST是一个简单的计算机视觉数据集。 它由像这样的手写数字的图像组成："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "korean-restoration",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG951.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-maria",
   "metadata": {},
   "source": [
    "每个图像是28*28像素的大小，在计算机，我们可以解释成一大批的数字："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "color-ballot",
   "metadata": {},
   "source": [
    "<img src=\"http://tianchi-media.oss-cn-beijing.aliyuncs.com/AIlearning/TensorFlow%E8%AF%BE%E7%A8%8B%E5%9B%BE%E7%89%87/%E7%9B%B8%E7%88%B1%E7%AF%87-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/WechatIMG952.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-retailer",
   "metadata": {},
   "source": [
    "  MNIST中的每个图像都具有相应的标签，0到9之间的数字表示图像中绘制的数字。我们将要将我们的标签称为“one-hot vectors”。 一个one-hot vectors是其他维数为0，单个维度为1的向量。 在这种情况下，第n个数字将被表示为在第n维中为1的向量。 例如，3将是[0,0,0,1,0,0,0,0,0,0].因此mnist.train.labels是一个[55000,10]的浮点数组。使用代码加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "smart-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #导入tensorflow库\n",
    "import tensorflow.keras as keras  #使用keras内置的数据集\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() #加载MNIST数据集\n",
    "print(x_train.shape)  #打印训练集的图片\n",
    "print(y_train.shape)  #打印训练集的标签\n",
    "print(x_test.shape)   #打印测试数据集的图片\n",
    "print(y_test.shape)   #打印测试集的标签\n",
    "#输入网络的训练集测试集以及相应的标签\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-engagement",
   "metadata": {},
   "source": [
    "## 3.2 模型 \n",
    "这里我们使用tensorflow2.0 的keras，在Sequential中搭建网络结构，这里的网络结构是输入层(28*28)，隐藏层(128层)，输出层(10),相当于走一遍前向传播."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "detailed-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-amendment",
   "metadata": {},
   "source": [
    "告诉训练时候选择了那种优化器，选择那种损失函数，选择那种评测指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "above-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', #选择adam优化器\n",
    "              loss='sparse_categorical_crossentropy',#选择sparse_categorical_crossentrop损失函数\n",
    "              metrics=['accuracy']) #选择sparse_categorical_accuracy评测指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-juice",
   "metadata": {},
   "source": [
    "## 3.3 训练\n",
    "     在训练的时候告诉训练集和测试集的输入特征和标签，batch_size大小，要迭代多少次epochs数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "overall-denial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5006 - accuracy: 0.8484 - val_loss: 0.1388 - val_accuracy: 0.9563\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 997us/step - loss: 0.1421 - accuracy: 0.9591 - val_loss: 0.0922 - val_accuracy: 0.9721\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0909 - accuracy: 0.9716 - val_loss: 0.0752 - val_accuracy: 0.9764\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0684 - accuracy: 0.9787 - val_loss: 0.0803 - val_accuracy: 0.9741\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 969us/step - loss: 0.0569 - accuracy: 0.9818 - val_loss: 0.0777 - val_accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x134261d50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-joining",
   "metadata": {},
   "source": [
    "打印网络结构和参数统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "minute-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-mistake",
   "metadata": {},
   "source": [
    "## 3.4 验证\n",
    "    测试训练集中的预测好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "possible-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 715us/step - loss: 0.0777 - accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07767903804779053, 0.977400004863739]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-salad",
   "metadata": {},
   "source": [
    "# 四、 课后习题\n",
    "   1 自行回顾感知机，全连接层，试着自己可以写出这样一个流程\n",
    "   2 换用keras自带的其他数据集，写一个神经网络实现的图像分类例子"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
