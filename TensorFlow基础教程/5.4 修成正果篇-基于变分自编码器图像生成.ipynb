{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 5.4修成正果篇-基于变分自编码器图像生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders（VAE)由Kingma et al.和Rezende et al.在2013年提出，它在图像生成、强化学习和自然语言处理等多个领域都有很广泛的应用.我们可以学习到给定 x，隐藏变量的条件概率分布P(z|x)，在学习到这个分布后，通过 对P(z|x)进行采样可以生成不同的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161829959015020561618299587169.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入𝒙通过编码器网络𝑞 ( | )计算得到隐变量 z 的均 值与方差，通过Reparameterizationtrick方式采样后送入解码器网络，获得分布𝑝𝜃( | )， 并通过公式计算误差并优化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、本节目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解VAE框架，编写VAE代码，能在自己的数据集上训练自己的图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用了Fashion MNIST数据集，包含了 10 类不同类型的衣服、鞋子、包等灰度图片，图片大小为 28x28，共 70000 张图片，其中 60000 张用于训练集，10000 张用于测试集，每行为一种类别图片。可以看到，Fashion MNIST 除了图片内容与 MNIST 不一样，其 它设定都相同，大部分情况可以直接替换掉原来基于 MNIST 训练的算法代码，而不需要 额外修改。由于 Fashion MNIST 图片识别相对于 MNIST 图片更难，因此可以用于测试稍复杂的算法性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras import Sequential, layers\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   超参数\n",
    "z_dim = 10\n",
    "h_dim = 20\n",
    "batchsz = 512\n",
    "learn_rate = 1e-3\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "print('x_train.shape:', x_train.shape)\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train).shuffle(batchsz * 5).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test).batch(batchsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161830827523983751618308264494.png\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "可以看到网络 输 入为 Fashion MNIST 图片向量，经过 3 个全连接层后得到隐向量𝐳的均值与方差，分别用 2 个输出节点数为 20 的全连接层表示，FC2 的 20 个输出节点表示 20 个特征分布的均值向量u\n",
    "，FC3 的 20 个输出节点表示 20 个特征分布的方差向量的𝑙𝑜𝑔值。通过 Reparameterization trick 采样获得长度为 20 的隐向量𝐳，并通过 FC4/FC5 重建出样本图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 编码器的搭建"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        #   Encoder\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(z_dim)      #       获得均值\n",
    "        self.fc3 = layers.Dense(z_dim)      #       获得均值\n",
    "\n",
    "        #   Decoder\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "def encoder(self,x):\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        #   get mean    获取均值\n",
    "        mu = self.fc2(h)\n",
    "        #   get variance    获取方差\n",
    "        log_var = self.fc3(h)\n",
    "\n",
    "        return mu,log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器的 模块就是下面的图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161830910638287101618309105937.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 解码器的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可对照模型自己找到对应位置"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def decoder(self,z):\n",
    "\n",
    "       out = tf.nn.relu(self.fc4(z))\n",
    "       out = self.fc5(out)\n",
    "\n",
    "       return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 模型拼接"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def call(self, inputs, training=None, mask=None):\n",
    "        #   [b,784] =>[b,z_dim],[b,z_dim]\n",
    "        #可以看到编码器的部分，输出均值和方差\n",
    "        mu,log_var = self.encoder(inputs)\n",
    "        ## 随机产生正态分布\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        #产生方差\n",
    "        std = tf.exp(log_var) ** 0.5\n",
    "        z = mu + std * eps\n",
    "\n",
    "        x_hat = self.decoder(z)\n",
    "        #解码器的部分，最终输出x_hat，预测输出，均值，方差\n",
    "        return x_hat,mu,log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3整体框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   超参数\n",
    "z_dim = 10\n",
    "h_dim = 20\n",
    "batchsz = 512\n",
    "learn_rate = 1e-3\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        #   Encoder\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(z_dim)      #       获得均值\n",
    "        self.fc3 = layers.Dense(z_dim)      #       获得均值\n",
    "\n",
    "        #   Decoder\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "\n",
    "    def encoder(self,x):\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        #   get mean    获取均值\n",
    "        mu = self.fc2(h)\n",
    "        #   get variance    获取方差\n",
    "        log_var = self.fc3(h)\n",
    "\n",
    "        return mu,log_var\n",
    "\n",
    "    def decoder(self,z):\n",
    "\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        #   [b,784] =>[b,z_dim],[b,z_dim]\n",
    "        mu,log_var = self.encoder(inputs)\n",
    "\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        std = tf.exp(log_var) ** 0.5\n",
    "        z = mu + std * eps\n",
    "\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat,mu,log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3网络的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = VAE()\n",
    "my_model.build(input_shape=(4,784))\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#整体训练100轮\n",
    "for epoch in range(100):\n",
    "#加载数据集\n",
    "    for step,x in enumerate(train_db):\n",
    "    #将28*28的reshape成784，方便输入\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        #定义梯度，# 构建梯度记录器\n",
    "        with tf.GradientTape() as  tape:\n",
    "            #输入x到模型中，输出样本，方差和均值，# 前向计算\n",
    "            x_hat,mu,log_var = my_model(x)\n",
    "           #损失计算\n",
    "            # rec_loss = tf.losses.binary_crossentropy(x, x_hat, from_logits=True)\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_hat)\n",
    "            #\n",
    "            rec_loss = tf.reduce_sum(rec_loss)/x.shape[0]\n",
    "           # 计算KL散度 N(mu, var) VS N(0, 1)\n",
    "            #   分布loss  (mu,var) - N(0,1)\n",
    "            kl_div = -0.5 * (log_var + 1 - mu ** 2 - tf.exp(log_var))\n",
    "            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "\n",
    "            #   两个误差综合\n",
    "            my_loss = rec_loss + 1. * kl_div\n",
    "        # 自动求导\n",
    "        grads = tape.gradient(my_loss, my_model.trainable_variables)\n",
    "        # 自动更新\n",
    "        opt.apply_gradients(zip(grads, my_model.trainable_variables))\n",
    "        # 打印训练误差\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,float(my_loss),'kl div:',float(kl_div),'rec loss:',float(rec_loss))\n",
    "\n",
    "        #   evaluation\n",
    "        #   随机Z只用decode生成\n",
    "        z = tf.random.normal((batchsz,z_dim))\n",
    "        logits = my_model.decoder(z)\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        my_save_img(x_hat,'{}_random'.format(epoch))\n",
    "\n",
    "        x = next(iter(test_db))\n",
    "        my_save_img(x, '{}_label'.format(epoch))\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        x_hat_logits, _, _ = my_model(x)\n",
    "        x_hat = tf.sigmoid(x_hat_logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        my_save_img(x_hat, '{}_pre'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 图片生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片生成只利用到解码器网络，首先从先验分布𝒩(0, 𝐼)中采样获得隐向量，再通过解\n",
    "码器获得图片向量，最后 Reshape 为图片矩阵:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        #从正态分布随机采样z\n",
    "        z = tf.random.normal((batchsz,z_dim))\n",
    "        #输入z正态分布生成，通过解码器生成图片\n",
    "        logits = my_model.decoder(z)\n",
    "        # # 转换为像素范围\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        #784  reshape成78*78\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        #保存图片 \n",
    "        my_save_img(x_hat,'{}_random'.format(epoch))\n",
    "        #重建图片，从测试集采样图片\n",
    "        x = next(iter(test_db))\n",
    "        \n",
    "        my_save_img(x, '{}_label'.format(epoch))\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        # 打平并送入自编码器\n",
    "        x_hat_logits, _, _ = my_model(x)\n",
    "        # 将输出转换为像素值\n",
    "        x_hat = tf.sigmoid(x_hat_logits)\n",
    "        # 恢复为 28x28\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255\n",
    "        my_save_img(x_hat, '{}_pre'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存的图片在下面的路径中找到，其中  1_label.jpg 是真实的图片，1_pre.jpg 是预测的图片， 1_random.jpg 是产生随机产生正态分布的图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161831058872214681618310584152.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 整体代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n",
      "0 0 548.4443359375 kl div: 2.5494801998138428 rec loss: 545.8948364257812\n",
      "0 100 307.4501037597656 kl div: 15.489289283752441 rec loss: 291.9608154296875\n",
      "1 0 299.75299072265625 kl div: 15.903350830078125 rec loss: 283.8496398925781\n",
      "1 100 275.8286437988281 kl div: 16.12905502319336 rec loss: 259.6995849609375\n",
      "2 0 271.4725036621094 kl div: 16.0245361328125 rec loss: 255.44796752929688\n",
      "2 100 268.268798828125 kl div: 15.50701904296875 rec loss: 252.7617950439453\n",
      "3 0 264.3460388183594 kl div: 15.03890323638916 rec loss: 249.30712890625\n",
      "3 100 262.34039306640625 kl div: 15.260851860046387 rec loss: 247.07952880859375\n",
      "4 0 253.7904510498047 kl div: 15.42512321472168 rec loss: 238.36532592773438\n",
      "4 100 257.94842529296875 kl div: 15.146361351013184 rec loss: 242.80206298828125\n",
      "5 0 252.0976104736328 kl div: 14.786120414733887 rec loss: 237.31149291992188\n",
      "5 100 258.8625793457031 kl div: 14.585002899169922 rec loss: 244.277587890625\n",
      "6 0 253.73342895507812 kl div: 15.036932945251465 rec loss: 238.69650268554688\n",
      "6 100 255.98248291015625 kl div: 14.916383743286133 rec loss: 241.06610107421875\n",
      "7 0 250.71055603027344 kl div: 15.591398239135742 rec loss: 235.11915588378906\n",
      "7 100 249.5076141357422 kl div: 15.29631233215332 rec loss: 234.2113037109375\n",
      "8 0 251.6667022705078 kl div: 15.108563423156738 rec loss: 236.55813598632812\n",
      "8 100 249.81063842773438 kl div: 14.965271949768066 rec loss: 234.84536743164062\n",
      "9 0 253.0771942138672 kl div: 14.987467765808105 rec loss: 238.0897216796875\n",
      "9 100 252.9612579345703 kl div: 15.01022720336914 rec loss: 237.95103454589844\n",
      "10 0 246.45578002929688 kl div: 15.542874336242676 rec loss: 230.91290283203125\n",
      "10 100 256.623291015625 kl div: 14.906011581420898 rec loss: 241.71728515625\n",
      "11 0 246.5287628173828 kl div: 15.200363159179688 rec loss: 231.32839965820312\n",
      "11 100 250.46685791015625 kl div: 14.560946464538574 rec loss: 235.90591430664062\n",
      "12 0 242.79124450683594 kl div: 15.042963981628418 rec loss: 227.74827575683594\n",
      "12 100 250.48402404785156 kl div: 14.75331974029541 rec loss: 235.73069763183594\n",
      "13 0 245.4562225341797 kl div: 14.869735717773438 rec loss: 230.58648681640625\n",
      "13 100 248.33828735351562 kl div: 15.028660774230957 rec loss: 233.30963134765625\n",
      "14 0 245.96087646484375 kl div: 15.02476692199707 rec loss: 230.9361114501953\n",
      "14 100 247.24856567382812 kl div: 14.683296203613281 rec loss: 232.56527709960938\n",
      "15 0 247.93731689453125 kl div: 14.796213150024414 rec loss: 233.14109802246094\n",
      "15 100 253.58502197265625 kl div: 14.429316520690918 rec loss: 239.15570068359375\n",
      "16 0 247.59039306640625 kl div: 14.734209060668945 rec loss: 232.85618591308594\n",
      "16 100 250.64096069335938 kl div: 15.375968933105469 rec loss: 235.26498413085938\n",
      "17 0 241.56625366210938 kl div: 14.72528076171875 rec loss: 226.84097290039062\n",
      "17 100 247.28981018066406 kl div: 14.910773277282715 rec loss: 232.37904357910156\n",
      "18 0 244.0305633544922 kl div: 15.132058143615723 rec loss: 228.89849853515625\n",
      "18 100 248.18048095703125 kl div: 14.950718879699707 rec loss: 233.22976684570312\n",
      "19 0 251.2783660888672 kl div: 14.645832061767578 rec loss: 236.63253784179688\n",
      "19 100 239.05044555664062 kl div: 14.873673439025879 rec loss: 224.17677307128906\n",
      "20 0 247.25265502929688 kl div: 14.90079116821289 rec loss: 232.35186767578125\n",
      "20 100 246.08255004882812 kl div: 14.836338996887207 rec loss: 231.2462158203125\n",
      "21 0 240.4307861328125 kl div: 15.049428939819336 rec loss: 225.38136291503906\n",
      "21 100 248.19406127929688 kl div: 15.370893478393555 rec loss: 232.8231658935547\n",
      "22 0 245.36184692382812 kl div: 14.668174743652344 rec loss: 230.69366455078125\n",
      "22 100 248.7467041015625 kl div: 14.982873916625977 rec loss: 233.76382446289062\n",
      "23 0 245.0465087890625 kl div: 14.924989700317383 rec loss: 230.12152099609375\n",
      "23 100 234.57907104492188 kl div: 14.838258743286133 rec loss: 219.74081420898438\n",
      "24 0 239.35679626464844 kl div: 14.710206985473633 rec loss: 224.64659118652344\n",
      "24 100 246.0581512451172 kl div: 14.923357963562012 rec loss: 231.13479614257812\n",
      "25 0 243.38352966308594 kl div: 15.248674392700195 rec loss: 228.13485717773438\n",
      "25 100 244.42662048339844 kl div: 15.074543952941895 rec loss: 229.35208129882812\n",
      "26 0 243.93397521972656 kl div: 15.092223167419434 rec loss: 228.8417510986328\n",
      "26 100 242.44393920898438 kl div: 14.940398216247559 rec loss: 227.5035400390625\n",
      "27 0 242.85731506347656 kl div: 14.773818969726562 rec loss: 228.08349609375\n",
      "27 100 242.87782287597656 kl div: 14.553975105285645 rec loss: 228.3238525390625\n",
      "28 0 245.09730529785156 kl div: 15.190876007080078 rec loss: 229.90643310546875\n",
      "28 100 239.9737091064453 kl div: 15.159610748291016 rec loss: 224.81410217285156\n",
      "29 0 244.97886657714844 kl div: 15.212288856506348 rec loss: 229.76657104492188\n",
      "29 100 247.9981689453125 kl div: 14.745617866516113 rec loss: 233.25254821777344\n",
      "30 0 244.20066833496094 kl div: 14.979875564575195 rec loss: 229.22079467773438\n",
      "30 100 240.43238830566406 kl div: 14.57848834991455 rec loss: 225.85389709472656\n",
      "31 0 242.91656494140625 kl div: 14.603123664855957 rec loss: 228.31344604492188\n",
      "31 100 242.71461486816406 kl div: 14.977396011352539 rec loss: 227.73721313476562\n",
      "32 0 243.86424255371094 kl div: 15.412911415100098 rec loss: 228.45132446289062\n",
      "32 100 242.91522216796875 kl div: 14.970390319824219 rec loss: 227.94483947753906\n",
      "33 0 242.86740112304688 kl div: 14.893121719360352 rec loss: 227.97427368164062\n",
      "33 100 245.5304412841797 kl div: 15.389063835144043 rec loss: 230.14137268066406\n",
      "34 0 240.02532958984375 kl div: 14.803901672363281 rec loss: 225.221435546875\n",
      "35 0 241.74717712402344 kl div: 15.391587257385254 rec loss: 226.3555908203125\n",
      "35 100 243.16506958007812 kl div: 14.88521957397461 rec loss: 228.27984619140625\n",
      "36 0 240.22543334960938 kl div: 15.256776809692383 rec loss: 224.96865844726562\n",
      "36 100 245.80972290039062 kl div: 15.318513870239258 rec loss: 230.4912109375\n",
      "37 0 246.06182861328125 kl div: 14.809423446655273 rec loss: 231.25241088867188\n",
      "37 100 245.40087890625 kl div: 15.236401557922363 rec loss: 230.1644744873047\n",
      "38 0 238.65493774414062 kl div: 15.261299133300781 rec loss: 223.39364624023438\n",
      "38 100 244.23216247558594 kl div: 15.373289108276367 rec loss: 228.85887145996094\n",
      "39 0 238.95877075195312 kl div: 14.844903945922852 rec loss: 224.11386108398438\n",
      "39 100 249.89498901367188 kl div: 15.22217082977295 rec loss: 234.67282104492188\n",
      "40 0 241.18585205078125 kl div: 14.907734870910645 rec loss: 226.2781219482422\n",
      "40 100 238.8284149169922 kl div: 15.433309555053711 rec loss: 223.39511108398438\n",
      "41 100 242.535888671875 kl div: 14.768157958984375 rec loss: 227.76773071289062\n",
      "42 0 243.67774963378906 kl div: 14.772167205810547 rec loss: 228.90557861328125\n",
      "42 100 246.23822021484375 kl div: 15.200422286987305 rec loss: 231.0377960205078\n",
      "43 0 246.65834045410156 kl div: 15.184768676757812 rec loss: 231.47357177734375\n",
      "43 100 244.76165771484375 kl div: 14.786598205566406 rec loss: 229.97506713867188\n",
      "44 0 238.97967529296875 kl div: 15.197967529296875 rec loss: 223.78170776367188\n",
      "44 100 246.78611755371094 kl div: 15.254253387451172 rec loss: 231.5318603515625\n",
      "45 0 241.8135223388672 kl div: 15.03813362121582 rec loss: 226.775390625\n",
      "45 100 240.57479858398438 kl div: 15.18606185913086 rec loss: 225.38873291015625\n",
      "46 0 234.4329071044922 kl div: 14.99402904510498 rec loss: 219.43887329101562\n",
      "46 100 241.30401611328125 kl div: 14.685820579528809 rec loss: 226.61819458007812\n",
      "47 0 236.05160522460938 kl div: 15.12966537475586 rec loss: 220.92193603515625\n",
      "47 100 246.25927734375 kl div: 15.007240295410156 rec loss: 231.25204467773438\n",
      "48 0 245.06442260742188 kl div: 15.24377155303955 rec loss: 229.82064819335938\n",
      "48 100 246.9999237060547 kl div: 14.989882469177246 rec loss: 232.01004028320312\n",
      "49 0 234.26023864746094 kl div: 15.278122901916504 rec loss: 218.98211669921875\n",
      "49 100 242.22296142578125 kl div: 14.79230785369873 rec loss: 227.43064880371094\n",
      "50 0 242.32859802246094 kl div: 14.99213695526123 rec loss: 227.33645629882812\n",
      "50 100 246.81983947753906 kl div: 14.948777198791504 rec loss: 231.87106323242188\n",
      "51 0 248.25807189941406 kl div: 15.478017807006836 rec loss: 232.78005981445312\n",
      "51 100 246.59619140625 kl div: 14.87643051147461 rec loss: 231.71975708007812\n",
      "52 0 240.55316162109375 kl div: 15.308907508850098 rec loss: 225.24424743652344\n",
      "52 100 241.0614013671875 kl div: 14.874320983886719 rec loss: 226.18707275390625\n",
      "53 0 236.95521545410156 kl div: 15.543856620788574 rec loss: 221.41136169433594\n",
      "54 0 241.66424560546875 kl div: 14.902626037597656 rec loss: 226.76162719726562\n",
      "54 100 244.76065063476562 kl div: 14.827460289001465 rec loss: 229.93319702148438\n",
      "55 0 243.56910705566406 kl div: 15.550302505493164 rec loss: 228.018798828125\n",
      "55 100 244.5153350830078 kl div: 15.232830047607422 rec loss: 229.28250122070312\n",
      "56 0 239.4245147705078 kl div: 14.550990104675293 rec loss: 224.87351989746094\n",
      "56 100 246.4626007080078 kl div: 15.187216758728027 rec loss: 231.275390625\n",
      "57 0 240.8386993408203 kl div: 14.307313919067383 rec loss: 226.53138732910156\n",
      "57 100 241.82337951660156 kl div: 14.947315216064453 rec loss: 226.87606811523438\n",
      "58 0 239.89051818847656 kl div: 15.1179838180542 rec loss: 224.7725372314453\n",
      "58 100 244.62632751464844 kl div: 15.277994155883789 rec loss: 229.34832763671875\n",
      "59 0 246.09913635253906 kl div: 15.111209869384766 rec loss: 230.98793029785156\n",
      "59 100 244.75946044921875 kl div: 14.734798431396484 rec loss: 230.024658203125\n",
      "60 0 238.8301239013672 kl div: 14.916400909423828 rec loss: 223.91372680664062\n",
      "60 100 243.15994262695312 kl div: 14.958213806152344 rec loss: 228.20172119140625\n",
      "61 0 237.97007751464844 kl div: 14.698785781860352 rec loss: 223.2712860107422\n",
      "61 100 241.24606323242188 kl div: 14.629348754882812 rec loss: 226.61671447753906\n",
      "62 0 237.2147674560547 kl div: 15.236799240112305 rec loss: 221.97796630859375\n",
      "62 100 241.14698791503906 kl div: 14.738296508789062 rec loss: 226.40869140625\n",
      "63 0 239.8520965576172 kl div: 15.127315521240234 rec loss: 224.7247772216797\n",
      "63 100 243.533203125 kl div: 15.063594818115234 rec loss: 228.4696044921875\n",
      "64 0 233.6703643798828 kl div: 15.237009048461914 rec loss: 218.433349609375\n",
      "64 100 240.98060607910156 kl div: 15.108572006225586 rec loss: 225.87203979492188\n",
      "65 100 241.26089477539062 kl div: 14.886054992675781 rec loss: 226.37484741210938\n",
      "66 100 240.9745330810547 kl div: 15.014878273010254 rec loss: 225.95965576171875\n",
      "67 0 239.11154174804688 kl div: 14.60538387298584 rec loss: 224.50616455078125\n",
      "67 100 247.38290405273438 kl div: 15.093727111816406 rec loss: 232.2891845703125\n",
      "68 0 238.612548828125 kl div: 15.018518447875977 rec loss: 223.59402465820312\n",
      "68 100 240.84036254882812 kl div: 14.961219787597656 rec loss: 225.87913513183594\n",
      "69 0 234.7095184326172 kl div: 14.90007495880127 rec loss: 219.8094482421875\n",
      "69 100 242.4503936767578 kl div: 14.60363483428955 rec loss: 227.8467559814453\n",
      "70 0 239.91644287109375 kl div: 15.353519439697266 rec loss: 224.56292724609375\n",
      "70 100 244.4234619140625 kl div: 14.486272811889648 rec loss: 229.93719482421875\n",
      "71 0 245.46316528320312 kl div: 14.985779762268066 rec loss: 230.47738647460938\n",
      "71 100 240.79476928710938 kl div: 15.044709205627441 rec loss: 225.75006103515625\n",
      "72 0 239.05279541015625 kl div: 14.81469440460205 rec loss: 224.23809814453125\n",
      "72 100 236.8790740966797 kl div: 15.11263656616211 rec loss: 221.7664337158203\n",
      "73 0 240.2605743408203 kl div: 15.10597038269043 rec loss: 225.15460205078125\n",
      "73 100 243.4962615966797 kl div: 15.161785125732422 rec loss: 228.33447265625\n",
      "74 0 236.7451171875 kl div: 15.11254596710205 rec loss: 221.632568359375\n",
      "75 0 246.60198974609375 kl div: 14.7181396484375 rec loss: 231.88385009765625\n",
      "75 100 240.70513916015625 kl div: 15.067039489746094 rec loss: 225.63809204101562\n",
      "76 0 235.0732421875 kl div: 15.018656730651855 rec loss: 220.05458068847656\n",
      "76 100 245.56687927246094 kl div: 14.749068260192871 rec loss: 230.81781005859375\n",
      "77 100 238.7007598876953 kl div: 15.028942108154297 rec loss: 223.67181396484375\n",
      "78 0 244.36021423339844 kl div: 15.613085746765137 rec loss: 228.74713134765625\n",
      "78 100 241.73243713378906 kl div: 14.98741626739502 rec loss: 226.74502563476562\n",
      "79 0 236.2093505859375 kl div: 15.260080337524414 rec loss: 220.9492645263672\n",
      "79 100 241.62313842773438 kl div: 14.97923469543457 rec loss: 226.64390563964844\n",
      "80 0 239.1473388671875 kl div: 14.340904235839844 rec loss: 224.80642700195312\n",
      "80 100 244.6103057861328 kl div: 15.274222373962402 rec loss: 229.33609008789062\n",
      "81 0 239.32493591308594 kl div: 14.749944686889648 rec loss: 224.5749969482422\n",
      "81 100 240.3233642578125 kl div: 15.034990310668945 rec loss: 225.2883758544922\n",
      "82 0 238.42617797851562 kl div: 14.516829490661621 rec loss: 223.9093475341797\n",
      "82 100 237.85186767578125 kl div: 14.771060943603516 rec loss: 223.080810546875\n",
      "83 100 236.10154724121094 kl div: 15.419904708862305 rec loss: 220.681640625\n",
      "84 0 239.80284118652344 kl div: 14.72311019897461 rec loss: 225.07972717285156\n",
      "84 100 244.43968200683594 kl div: 15.101421356201172 rec loss: 229.3382568359375\n",
      "85 0 241.72921752929688 kl div: 15.20702075958252 rec loss: 226.52220153808594\n",
      "85 100 240.29954528808594 kl div: 15.01087760925293 rec loss: 225.28866577148438\n",
      "86 0 233.9630126953125 kl div: 14.814117431640625 rec loss: 219.14889526367188\n",
      "86 100 240.70794677734375 kl div: 14.813822746276855 rec loss: 225.8941192626953\n",
      "87 0 237.40208435058594 kl div: 15.064350128173828 rec loss: 222.33773803710938\n",
      "87 100 247.61366271972656 kl div: 14.97809886932373 rec loss: 232.63555908203125\n",
      "88 100 245.61788940429688 kl div: 14.94862174987793 rec loss: 230.6692657470703\n",
      "89 100 244.73529052734375 kl div: 14.944694519042969 rec loss: 229.7906036376953\n",
      "90 0 236.63446044921875 kl div: 15.402756690979004 rec loss: 221.23170471191406\n",
      "90 100 241.2864532470703 kl div: 14.762802124023438 rec loss: 226.52365112304688\n",
      "91 0 239.40451049804688 kl div: 15.098099708557129 rec loss: 224.30641174316406\n",
      "91 100 241.2630157470703 kl div: 14.869194984436035 rec loss: 226.39381408691406\n",
      "92 0 241.13909912109375 kl div: 14.788492202758789 rec loss: 226.35060119628906\n",
      "92 100 243.13272094726562 kl div: 15.182747840881348 rec loss: 227.94996643066406\n",
      "93 0 237.88348388671875 kl div: 14.919438362121582 rec loss: 222.96405029296875\n",
      "93 100 248.71463012695312 kl div: 15.211087226867676 rec loss: 233.5035400390625\n",
      "94 0 239.9485626220703 kl div: 15.35197639465332 rec loss: 224.59658813476562\n",
      "94 100 240.25140380859375 kl div: 14.575958251953125 rec loss: 225.67544555664062\n",
      "95 0 241.6166534423828 kl div: 14.580302238464355 rec loss: 227.03634643554688\n",
      "95 100 240.41714477539062 kl div: 14.758415222167969 rec loss: 225.65872192382812\n",
      "96 0 237.50131225585938 kl div: 15.244438171386719 rec loss: 222.2568817138672\n",
      "96 100 241.4024200439453 kl div: 15.284652709960938 rec loss: 226.11776733398438\n",
      "97 0 234.79299926757812 kl div: 14.766534805297852 rec loss: 220.02645874023438\n",
      "97 100 236.2447052001953 kl div: 15.074361801147461 rec loss: 221.17034912109375\n",
      "98 0 240.8311309814453 kl div: 14.80811595916748 rec loss: 226.02301025390625\n",
      "98 100 239.38507080078125 kl div: 14.960487365722656 rec loss: 224.42457580566406\n",
      "99 0 234.47726440429688 kl div: 15.234512329101562 rec loss: 219.2427520751953\n",
      "99 100 236.4609832763672 kl div: 15.01609992980957 rec loss: 221.44488525390625\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras import Sequential, layers\n",
    "import sys\n",
    "\n",
    "\n",
    "def my_save_img(data,name):\n",
    "    save_img_path = 'VAEimage/{}.jpg'.format(name)\n",
    "    new_img = np.zeros((280,280))\n",
    "    for index,each_img in enumerate(data[:100]):\n",
    "        row_start = int(index/10) * 28\n",
    "        col_start = (index%10)*28\n",
    "        # print(index,row_start,col_start)\n",
    "        new_img[row_start:row_start+28,col_start:col_start+28] = each_img\n",
    "\n",
    "    plt.imsave(save_img_path,new_img)\n",
    "\n",
    "#   超参数\n",
    "z_dim = 10\n",
    "h_dim = 20\n",
    "batchsz = 512\n",
    "learn_rate = 1e-3\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "print('x_train.shape:', x_train.shape)\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train).shuffle(batchsz * 5).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test).batch(batchsz)\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        #   Encoder\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(z_dim)      #       获得均值\n",
    "        self.fc3 = layers.Dense(z_dim)      #       获得均值\n",
    "\n",
    "        #   Decoder\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "\n",
    "    def encoder(self,x):\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        #   get mean    获取均值\n",
    "        mu = self.fc2(h)\n",
    "        #   get variance    获取方差\n",
    "        log_var = self.fc3(h)\n",
    "\n",
    "        return mu,log_var\n",
    "\n",
    "    def decoder(self,z):\n",
    "\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        #   [b,784] =>[b,z_dim],[b,z_dim]\n",
    "        mu,log_var = self.encoder(inputs)\n",
    "\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        std = tf.exp(log_var) ** 0.5\n",
    "        z = mu + std * eps\n",
    "\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat,mu,log_var\n",
    "\n",
    "my_model = VAE()\n",
    "# my_model.build(input_shape=(4,784))\n",
    "opt = tf.optimizers.Adam(learn_rate)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step,x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        with tf.GradientTape() as  tape:\n",
    "            x_hat,mu,log_var = my_model(x)\n",
    "\n",
    "            # rec_loss = tf.losses.binary_crossentropy(x, x_hat, from_logits=True)\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_hat)\n",
    "            rec_loss = tf.reduce_sum(rec_loss)/x.shape[0]\n",
    "\n",
    "            #   分布loss  (mu,var) - N(0,1)\n",
    "            kl_div = -0.5 * (log_var + 1 - mu ** 2 - tf.exp(log_var))\n",
    "            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "\n",
    "            #   两个误差综合\n",
    "            my_loss = rec_loss + 1. * kl_div\n",
    "\n",
    "        grads = tape.gradient(my_loss, my_model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, my_model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,float(my_loss),'kl div:',float(kl_div),'rec loss:',float(rec_loss))\n",
    "\n",
    "        #   evaluation\n",
    "        #   随机Z只用decode生成\n",
    "        z = tf.random.normal((batchsz,z_dim))\n",
    "        logits = my_model.decoder(z)\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        my_save_img(x_hat,'{}_random'.format(epoch))\n",
    "\n",
    "        x = next(iter(test_db))\n",
    "        my_save_img(x, '{}_label'.format(epoch))\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        x_hat_logits, _, _ = my_model(x)\n",
    "        x_hat = tf.sigmoid(x_hat_logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        my_save_img(x_hat, '{}_pre'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、课后作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 自己设置一个深度的VAE，完成自己数据集的重建\n",
    "\n",
    "2 掌握网络的输入输出维度变换"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "187042",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
