{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressing-killer",
   "metadata": {},
   "source": [
    "# 📚 4.3相爱篇-常用数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-snake",
   "metadata": {},
   "source": [
    "         ✅希望阳光很暖，微风不燥，时光不老，你我都好\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-defensive",
   "metadata": {},
   "source": [
    "        我们已经会一些基础的操作了：会数据类型，可以创建张量，序列，会索引切片，\n",
    "        可以对维度进行转换，交换，会前向传播，会数据统计，这些都是将图片，文本等一些数据集的表示，那么数据集在哪里呢，本小节将讲解数据集的加载。\n",
    "        我们先正式介绍对于常用的数据集，如何利用 TensorFlow 提供的工 具便捷地加载数据集。对于自定义的数据集的加载，我们会在后续章节介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-breathing",
   "metadata": {},
   "source": [
    "# 一、本节目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-joseph",
   "metadata": {},
   "source": [
    "    熟练掌握数据集的加载，因为数据集是模型训练的基石，没有数据集，什么也干不了，知道tensorflow2框架里面有哪些数据集，怎么加载数据集，怎么对加载过得数据集进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-thunder",
   "metadata": {},
   "source": [
    "# 二、常用数据集介绍\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-windows",
   "metadata": {},
   "source": [
    "        在 TensorFlow 中，keras.datasets 模块提供了常用经典数据集的自动下载、管理、加载 与转换功能，并且提供了 tf.data.Dataset 数据集对象，方便实现多线程(Multi-thread)，预处 理(Preprocess)，随机打散(Shuffle)和批训练(Train on batch)等常用数据集功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-methodology",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161612174510719531616121744512.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-matrix",
   "metadata": {},
   "source": [
    "常用的数据集，如：\n",
    "\n",
    "\n",
    "❑ Boston Housing 波士顿房价趋势数据集，用于回归模型训练与测试\n",
    "\n",
    "❑ CIFAR10/100 真实图片数据集，用于图片分类任务\n",
    "\n",
    "❑ MNIST/Fashion_MNIST 手写数字图片数据集，用于图片分类任务\n",
    "\n",
    "❑ IMDB 情感分类任务数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-shelter",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161612156166827631616121556271.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-forge",
   "metadata": {},
   "source": [
    "    这些数据集在机器学习、深度学习的研究和学习中使用的非常频繁。对于新提出的算法， 一般优先在简单的数据集上面测试，再尝试迁移到更大规模、更复杂的数据集上。通过 datasets.xxx.load_data()即可实现经典数据集的自动加载，其中 xxx 代表具体的数据集名称."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (60000, 28, 28) y: (60000,) x test: (10000, 28, 28) y test: [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets # 导入经典数据集加载模块\n",
    "# 加载 MNIST 数据集，每个数字的图片像素是28x28，通道数为1，70k的图片数量分为60k训练集和10k的测试集\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "print('x:', x.shape, 'y:', y.shape, 'x test:', x_test.shape, 'y test:', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternative-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253\n",
      "  159  50   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252\n",
      "  252 237   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239\n",
      "  233 252  57   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202\n",
      "   84 252 253 122   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252\n",
      "   96 189 253 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "   47  79 255 168   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21\n",
      "    0   0 253 243  50   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0\n",
      "    0   0 253 252 165   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0\n",
      "    0   0 255 253 196   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0\n",
      "    0   0 253 252 148   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0\n",
      "    7 135 253 186  12   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7\n",
      "  131 252 225  71   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      "  252 173   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253\n",
      "  162   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167\n",
      "   56   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "(60000, 28, 28)\n",
      "[5 0 4 1 9]\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x[1])\n",
    "print(x.shape)\n",
    "print(y[:5])    #可以看到下面数据集中前五个label y一次是5，0，4，1，9\n",
    "y_onehot = tf.one_hot(y, depth=10) #对标签进行预处理：one_hot编码\n",
    "print(y_onehot[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-imagination",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161612178609436201616121784684.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-prisoner",
   "metadata": {},
   "source": [
    "通过 load_data()会返回相应格式的数据，对于图片数据集 MNIST, CIFAR10 等，会返回 2 个 tuple，第一个 tuple 保存了用于训练的数据 x,y 训练集对象;第 2 个 tuple 则保存了用于 测试的数据 x_test,y_test 测试集对象，所有的数据都用 Numpy.array 容器承载。\n",
    "\n",
    "\n",
    "\n",
    "TensorFlow 会默认将数据缓存在用户目录的.keras/datasets 文件夹，用户不需要关心数据集是如何保存的。如果当前数据集不在缓存中，则会自动从网 站下载和解压，加载;如果已经在缓存中，自动完成加载:我的电脑看下图，完成默认缓存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-disability",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161612185491296681616121854735.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-remains",
   "metadata": {},
   "source": [
    "可以 看到进入datases目录 ，可以看到你需要用到的常用经典数据集的下载 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-timber",
   "metadata": {},
   "source": [
    "<img src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161612191396155081616121913689.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-natural",
   "metadata": {},
   "source": [
    "数据加载进入内存后，需要转换成 Dataset 对象，以利用 TensorFlow 提供的各种便捷 功能。通过 Dataset.from_tensor_slices 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "insured-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.uint8, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))#转换为tensor\n",
    "print(train_db)#得到一个数据集对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-petroleum",
   "metadata": {},
   "source": [
    "将数据转换成 Dataset 对象后，一般需要再添加一系列的数据集标准处理步骤，如随机打散，预处理，批处理等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-census",
   "metadata": {},
   "source": [
    "## 2.1 常用数据集标准处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-contamination",
   "metadata": {},
   "source": [
    "### 2.1.1 随机打散"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-slide",
   "metadata": {},
   "source": [
    "通过 Dataset.shuffle(buffer_size)工具可以设置 Dataset 对象随机打散数据之间的顺序，防止每次训练时数据按固定顺序产生，从而使得模型尝试“记忆”住标签信息，在做DP时，神经网络具有很强的记忆功能，如果总是按照固定的顺序进行训练，就会导致网络找到捷径影响预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "union-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "trai = train_db.shuffle(10000) #buffer_size 指定缓冲池的大小，一般设置为一个较大的参数即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-perspective",
   "metadata": {},
   "source": [
    "### 2.1.2 批训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-league",
   "metadata": {},
   "source": [
    "为了利用显卡的并行计算能力，一般在网络的计算过程中会同时计算多个样本，我们把这种训练方式叫做批训练，其中样本的数量叫做 batch size。为了一次能够从 Dataset 中产生 batch size 数量的样本，需要设置 Dataset 为批训练方式:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ancient-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = train_db.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-reform",
   "metadata": {},
   "source": [
    "其中 128 为 batch size 参数，即一次并行计算 128 个样本的数据。Batch size 一般根据用户 的 GPU 显存资源来设置，当显存不足时，可以适量减少 batch size 来减少算法的显存使用量。比如现在有6400个数据集，dataset，现在批训练设置为128，一个epoch需要迭代50次，一次性并行计算 128 个样本的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-franchise",
   "metadata": {},
   "source": [
    "### 2.1.2 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-contribution",
   "metadata": {},
   "source": [
    "从 keras.datasets 中加载的数据集的格式大部分情况都不能满足模型的输入要求，因此需要根据用户的逻辑自己实现预处理函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "terminal-maker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义的预处理函数，这里面可以对图像进行裁剪，旋转等一系列操作\n",
    "def preProcess(x,y):\n",
    "    x = tf.cast(x, dtype = tf.float32)/255\n",
    "    y = tf.cast(y, dtype = tf.int32)\n",
    "    y = tf.one_hot(y,depth = 10)\n",
    "    return x, y\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db2 = db.map(preProcess)#\n",
    "b=next(iter(db2))#iter(db2)得到迭代器，使用next方法得到一张图片\n",
    "b[1] #图片标签表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-princess",
   "metadata": {},
   "source": [
    "# 三、数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-affiliate",
   "metadata": {},
   "source": [
    "数据增强(Data Augmentation)：是指对图片进行随机的旋转、翻转、裁剪、随机设置图片的亮度和对比度以及对数据进行标准化(数据的均值为0，方差为1)。通过这些操作，我们可以获得更多的图片样本，原来的一张图片可以变为多张图片，扩大了样本容量，对于提高模型的准确率和提升模型的泛化能力非常有帮助，在进行数据增强的同时也会需要消耗大量的系统资源。常见的数据强化操作如下：\n",
    "\n",
    "• 旋转 | 反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向;\n",
    "\n",
    "• 翻转变换(flip): 沿着水平或者垂直方向翻转图像;\n",
    "\n",
    "• 缩放变换(zoom): 按照一定的比例放大或者缩小图像;\n",
    "\n",
    "• 平移变换(shift): 在图像平面上对图像以一定方式进行平移;可以采用随机或人为定义的方式指定平移范围和平移步长, 沿水平或竖直方向进行平移. 改变图像内容的位置;\n",
    "\n",
    "• 尺度变换(scale): 对图像按照指定的尺度因子, 进行放大或缩小; 或者参照SIFT特征提取思想, 利用指定的尺度因子对图像滤波构造尺度空间. 改变图像内容的大小或模糊程度;\n",
    "\n",
    "• 对比度变换(contrast): 在图像的HSV颜色空间，改变饱和度S和V亮度分量，保持色调H不变. 对每个像素的S和V分量进行指数运算(指数因子在0.25到4之间), 增加光照变化;\n",
    "\n",
    "• 噪声扰动(noise): 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;\n",
    "\n",
    "• 错切变换（shear）：效果就是让所有点的x坐标(或者y坐标)保持不变，而对应的y坐标(或者x坐标)则按比例生平移，且平移的大小和该点到x轴(或y轴)的垂直距离成正比。\n",
    "\n",
    "tf.keras.preprocessing.image.ImageDataGenerator()  # 用来创建一个图片数据集迭代器，可以在传入数据时进行一些数据加强操作.\n",
    "\n",
    "rescale：缩放因子，图像像素值乘以rescale，如果为255可以将像素值归一到0-1区间。\n",
    "featurewise_center：特征均值化，若为True，对输入的图片每个通道减去每个通道对应均值，与rescale=1/255相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-storage",
   "metadata": {},
   "source": [
    "    samplewise_center：样本均值化。\n",
    "    rotation_range：随机旋转角度范围\n",
    "    zca_epsilon:ZCA 白化的 epsilon 值，默认为 1e-6。\n",
    "    zca_whitening: 布尔值。是否应用 ZCA 白化。\n",
    "    rotation_range: 整数。随机旋转的度数范围。\n",
    "    width_shift_range：随机宽度偏移量\n",
    "    height_shift_range：随机高度偏移量\n",
    "    height_shift_range：水平翻转量\n",
    "    zoom_range：随机缩放，范围[1-n,1+n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-farming",
   "metadata": {},
   "source": [
    "#  四、课后作业"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-director",
   "metadata": {},
   "source": [
    "      1 知道数据处理在preprocessing里面，会使用里面每一个模块\n",
    "      2 对里面的函数进行图像的处理，进行数据增强操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
